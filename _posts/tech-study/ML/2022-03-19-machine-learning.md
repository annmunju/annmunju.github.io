---
title: 머신러닝 이론 및 데이터 처리
description: 기본 개념 공부 정리
author: annmunju
date: 2022-03-19 12:05:00 +0900
categories: [기술 공부 기록, ML]
tags: [ml, data, preprocessing]
pin: false
math: true
mermaid: false
comments: false
---


1. 데이터 전처리 : 데이터 생성, 정제, 변환, 결합
    1) 결측값 처리법
        - 완전 제거법 (정보 손실의 분석 결과 왜곡 가능성 있음)
        - 평균대체법 (추정량 표준오차가 과소 추정될 수 있음)
        - 핫덱대체법 : 동일한 데이터 내 결측값이 발생한 관찰치와 유사한 특성을 가진 다른 관찰치의 정보를 이용하여 대체하는 방법
    2) 이상치 판별
        - 박스플롯을 통해 Q1-1.5*IQR과 Q3+1.5*IQR의 범위를 넘어가는 자료를 이상값으로 진단
        - 표준화 점수(Z-score)의 절대값이 2, 3보다 큰 경우 이상값으로 진단
    3) 이상값 처리
        - 이상값 제외
        - 이상값 대체 : 이상값을 정상 범위의 최소값, 최대값으로 대체
        - 변수 변환 : 자료값 전체에 로그변환, 제곱근 변환 등 적용
    4) 연속형 자료 범주화
        - 방법 : 연속형 변수를 구간으로 나누어 범주화하여 이상치 문제 완화
        - 효과 : 이상치 문제 완화, 결측치 처리 방법, 과적합 방지 및 결과 해석 용이
    5) 데이터 변환
        - 목적 : 분포의 대칭화(정규화), 그룹간 비슷한 산포 유지, 변수간 관계 단순화(비선형→선형) 목적
        - 유형
            - 제곱근 변환 (왼쪽 꼬리 길게), 제곱 변환 (오른쪽 꼬리 길게)
            - 로그 변환(왼쪽 꼬리 길게), 지수 변환(오른쪽 꼬리 길게)
            - 박스콕스 변환 : 제곱근 또는 제곱 유형 변환의 일반화
    6) 데이터 결합(join)
        - inner join, full outer join, left join, right join

2. 머신러닝
    1) 개념 : 데이터를 스스로 학습시켜 문제를 해결할 수 있게 하는 기술
    2) 발전 : 머신러닝 알고리즘 발전 + 컴퓨팅 성능 향상 + 대용량 데이터 축적 및 관리 기술 발전 → 활용 증가
    3) 방법론 (분류)

        - 지도학습 : 훈련 데이터의 여러 특성변수를 이용해 목표변수를 예측하도록 모델을 학습
            - 회귀 알고리즘 : 목표변수(라벨)가 연속형 (Linear Regression, Decision Tree, SVM, Random Forest, Boosting, Neural Network, Deep Learning 등)
            - 분류 알고리즘 : 목표변수(라벨)이 범주형 (k-nearest Neighbors, Logistic Regression, Softmax regression 등)
        - 비지도학습(자율학습) : 라벨 없는 데이터에서 변수간 관계나 유사성을 바탕으로 의미있는 패턴 추출
            - 군집화 : 클러스터링 (k-means Clustering)
            - 차원축소 : 특성변수가 너무 많을 때 (PCA 등)
            - 추천시스템 : recommendation
        - 강화학습 : 행동 주체와 상태-보상이 바뀌는 환경 (SARSA, Q-Learning 등)

    4) 분석 절차
        - 데이터 전처리 및 탐색
        - 적절한 모델 선택
        - 데이터로 모델 학습
        - 훈련된 모델로 새로운 데이터 예측

    5) 검증 및 평가 : 과대적합 방지 및 일반화 오차 최소화 방법
        - Hold-out 방식 : 데이터를 Training 60% (Validation 20%,) Test 20%로 분리 (비율은 조정 가능)
            - 검증 데이터(Validation data) : 하이퍼파라미터 조정, 변수 선택, 모델 튜닝에 사용
            - 평가 지표로 train, test 데이터 비교 후 차이를 통해 모델 적합성 판단
        - k-fold 교차검증(Cross-validation) : 데이터가 적을 때, k개로 분할하여 train(k-1), test- 번갈아가며 전체 반복, 예측오차의 평균 계산

    6) 일반화 오차 및 편향-분산 Trade off : 과대/과소적합의 문제
        - 모델이 단순하면 예측 오차 커짐(편향↑, 분산↓)
        - 모델이 복잡하면 예측 오차 커짐(편향↓, 분산↑)
        - 일반화 오차 = 편향^2 + 분산
        - 과대적합 방지를 위해 데이터 양 증가, 모델 복잡도 감소(변수 영향력/차원 축소/규제 등)

3. 머신러닝 모델의 평가지표
    1) 회귀 모델
        - RMSE (Root Mean Square Error) : 오차 제곱의 평균에 제곱근을 씌운 값
        - R-square(결정계수) : 0은 오차가 크고, 1은 오차가 거의 없음
        - MAE(Mean Absolute Error) : 오차 절대값의 평균
        - MAPE(Mean Average Percentage Error) : 실제 값 대비 오차 비율의 평균

    2) 분류 모델
        (0) 정오분류표(confusion matrix)
        - 정확도(Accuracy) : 맞춘 경우 / 전체 경우
        - 정밀도 : 실제 Positive / 예측 Positive 전체
        - 재현율 : 예측 Positive / 실제 Positive 전체
        - ROC(Receiver Operating Characteristic)
            - AUC : ROC 곡선 아래 면적, 1에 가까울수록 좋은 모델

4. 특성 공학
    - 특성 공간의 차원을 축소해야 함 : 모델 해석력 향상, 훈련시간 단축, 차원의 저주 방지, 과적합 감소
    1) 특성 선택 : 주어진 특성 변수 중 가장 좋은 변수만 선택
        - filter : 각 특성과 목표변수의 연관성(1:1) 평가 (t-test, chi-square, information gain 등)
        - wrapper : 다양한 변수 조합으로 모델 학습·평가 (forward, backward, stepwise selection 등)
        - embedded : 알고리즘 내 직접 변수 최적화 (Ridge, Lasso, Elastic net 등)

    2) 특성 추출(차원축소법, 비지도학습)
        - 주성분 분석(PCA) : 변수들에 담긴 정보를 최대한 확보해 새로운 변수(주성분) 생성, 변동이 큰 축 탐색, 손실 최소화, 상관 없는 주성분으로 해석 용이
        - 특성값 분해(SVD) : 정보량이 많은 순서대로 m개만 이용해 데이터 근사

5. 비지도학습 - 군집분석 (차원축소법도 비지도학습)
    1) 개요 : 유사한 특성을 지닌 개체들을 몇 개의 군집(클러스터)으로 집단화. 특이 군집, 결측값 보정 등에도 사용
    2) 계층적 군집분석 : 병합적(가까운 것 순차적 묶기), 분할적(먼 것 나누기)
        - 시간이 오래 걸림. 다른 군집 분석의 군집 수 선정이나 일부만 미리 확인용으로 사용

        [거리 정의]
        - 개체 간 거리
            - 유클리디안, 맨해튼, 민코우스키 거리
            - 유클리디안 거리 : sqrt((a1-b1)^2 + (a2-b2)^2)
        - 군집 간 거리
            - 단일 연결법(최단 연결법): 가장 가까운 요소들 거리
            - 완전 연결법(최장 연결법): 가장 먼 요소들 거리
            - 평균 연결법: 모든 요소 간 거리의 평균
            - 중심 연결법: 군집 중심 간 거리
            - ward 연결법: 군집 중심에서 개체간 거리의 제곱합(SSEk), 전체 sum of squares 증가량으로 거리 정의

        - 덴드로그램 : 개체별 클러스터링 과정을 시각화한 그림

    3) 비계층적 군집분석
        - K-means 군집분석 : 사전 결정된 k만큼 군집 형성, 계산량 적고 대용량 데이터에 빠름. 초기 군집 중심 위치, 이상치 영향에 민감
        - k-means 알고리즘 : k개로 초기 군집화 → 군집 중심 계산 후 가장 가까운 군집에 할당 → 중심 재계산 → 반복
        - 군집 수 결정 : 군집수 k별 SSE(오차제곱합) 변화 관찰, SSE가 급격히 감소하다 완만해지는 지점의 k 선정

6. 지도학습 - 회귀분석 : 독립변수-종속변수 간 함수적 관련성을 밝히기 위해 수학적 모형을 세워, 이를 통계적으로 추정하는 분석 방법
    1) 단순선형회귀분석
        - 정의 : 독립변수 1개, 오차항은 독립적인 정규·등분산·독립 가정의 확률변수
        - 모수 의미 : α(알파), β(베타) 회귀계수, ε(엡실론) 오차항
        - 모수 추정(최소제곱법) : 수직거리 제곱합 SS(α, β)가 최소가 되도록 추정
        - 모수에 관한 가설검정(t 검정)
            - 귀무가설 H0 : β=0 (X와 Y 무관)
            - 대립가설 H1 : β≠0 (X와 Y 관련)
            - T통계량 및 p-value(0.05 이하면 유의성 검증됨)
        - 적합도(R-square)
            - 제곱합 : SST(yi 변동) = SSR(모형 설명변동) + SSE(잔차)
            - R² = SSR/SST = 1 - SSE/SST (r^2, 0~1)

    2) 다중선형회귀분석
        - 정의 : 독립변수 2개 이상
        - 모수 의미 : α, β₁... 회귀계수, ε 오차항
        - 모수 추정법 : 최소제곱법
        - 적합도(R-square) : 단순회귀와 동일, 범주형 독립변수는 더미변수 사용
        - 변수선택(wrapper) : forward, backward, stepwise
            - 전진선택법 : 중요한 변수부터 순차 추가(한번 추가는 제거 불가)
            - 후진제거법 : 중요하지 않은 변수 순차 제거(한번 제거는 선택 불가)
            - 단계별방법 : 중요 변수 추가/비중요 변수 제거를 반복(부분 F검정)
            - 모형선택 기준 : 수정된 결정계수(Adjusted R²)
        - 잔차(=오차 추정치) 분석
            - 오차 가정: 정규성, 등분산성, 독립성
            - 정규성 위반 → 변수변환, 등분산성 위반 → 가중최소제곱회귀, 독립성 위반 → 시계열분석
            - 잔차 분석 방법 : 검정/시각(히스토그램, QQ플롯, 산점도)
        - 다중공선성 : 독립변수 간 강한 상관관계로 회귀계수 추정에 부정적 영향
            - VIF계수 : 원하는 변수 Xj를 제외한 나머지 변수로 Rj² 산출
            - VIF 5(80%) or 10(90%) 이상이면 다중공선성 문제
            - 해결 : 변수선택, 주성분분석, 릿지/라쏘 등으로 영향 통제

    3) 규제가 있는 선형회귀모델
        : 모델 과적합 방지를 위한 규제 (릿지, 라쏘, 엘라스틱넷)
        - 규제 방식 : 회귀계수 크기를 제한 (Lp norm = p√(Σ |βj|^p))
        - 릿지(Ridge) : P=2(L2 norm), 비용함수에 λ*L2 norm 추가
            - λ 크면 규제↑, 회귀계수↓. 0이면 일반선형회귀와 동일. 적정 λ는 교차검증으로 찾음.
            - 릿지회귀는 제약범위가 원형이라 Bj가 0이 되지 않고 모두 줄어듦
        - 라쏘(Lasso) : P=1(L1 norm), 비용함수에 λ*L1 norm 추가
            - λ 크면 규제↑, 특정 Bj가 0이 될 수 있음(일부 변수 영향력 사라짐)
        - 엘라스틱넷 : 릿지+라쏘를 혼합, 두 가지 규제의 장점 모두 가짐. 추정은 더 복잡함

7. 지도학습 - 분류분석
    1) 로지스틱 회귀 : 종속변수가 범주형일 때, 새 설명변수 값에 대해 반응변수 각각의 범주 확률을 추정하고, cutoff 값으로 분류
        - 이항 로지스틱 회귀 : 이진 반응변수를 회귀식 형태로 예측
            - 로그오즈 : 관심사건과 비관심사건의 확률 비율을 log로 변환
            - 로그오즈를 쓰는 이유 : 확률 p는 0~1이지만, 독립변수 선형함수는 (-∞, ∞) 범위이기 때문
            - p=1/2이면 오즈는 1
            - 설명변수 1개일 때 p는 0~1 (x=-∞→p=0, x=∞→p=1)
            - 추정은 최대우도추정법, 경사하강법 등 사용. 새로운 자료 입력시 p값 계산, 기준(threshold) 이상이면 1, 이하면 0 분류
            - 분리경계면 : 변수 공간에서 Y값 결정 경계
            - 오즈비 : X1만 1 증가시켰을 때 오즈 변화 = exp(β1), β1(+)면 양의 상관, β1(-)면 음의 상관
        - 나이브베이즈 : 생성모델의 일종
            - 목표 Y가 2개일 때, X값으로 Y분류
                - 조건부확률 : P[C1|x]>P[C2|x] → C1로 분류, 불만족시 C2
                - 베이즈정리: P[x|C1]*P[C1] > P[x|C2]*P[C2]
            - Y가 n개일 때 각 변수 독립 가정(매우 단순)
            - 장점: 연산 속도 빠름, 학습 데이터 적어도 성능 좋음, 다양한 분류·추천에 활용
            - 단점: 빈도0, 언더플로우(Underflow), 독립가정이 현실적으로 나이브함
        - KNN (K-Nearest-Neighbor)
            - 가장 가까운 k개 데이터를 바탕으로 분류/예측. K값 중요
                - k가 작으면 이상점 등 노이즈에 민감(과적합 위험)
                - k가 크면 패턴 파악 어려워 성능 저하
            - 거리 측정 : 유클리디안(제곱+제곱근), 맨해튼(절대값), 민코우스키(절대값합의 n제곱근)
            - 스케일 다르면 Z score 변환, min-max 변환 등 전처리 필요
