---
title: Learning Transferable Visual Models From Natural Language Supervision, 2021
description: Multimodal Model Review
author: annmunju
date: 2023-11-20 11:05:00 +0900
categories: [ê¸°ìˆ  ê³µë¶€ ê¸°ë¡, Paper]
tags: [dl, ai, multi-modal, paper]
pin: false
math: true
mermaid: false
comments: false
---

ğŸ“– Alec Radford, Jong Wook Kim, et al. Learning Transferable Visual Models From Natural Language Supervision, 2021.

## 0. ìš”ì•½

- ê¸°ì¡´ ì—°êµ¬
    - ë¯¸ë¦¬ ê²°ì •ëœ ê°ì²´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨
- CLIPì´ ê¸°ì¡´ ì—°êµ¬ì™€ ë‹¤ë¥¸ ì 
    - ì´ë¯¸ì§€ì— ëŒ€í•œ ì›ì‹œ í…ìŠ¤íŠ¸ í•™ìŠµì„ í†µí•´ì„œ ë” ê´‘ë²”ìœ„í•˜ê²Œ í•™ìŠµí•¨.
    - ì¸í„°ë„·ìœ¼ë¡œ ìˆ˜ì§‘í•œ 4ì–µ ìŒì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¡œ í›ˆë ¨
    - ì‚¬ì „ í›ˆë ¨ í›„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ê°€ëŠ¥
        - zero-shot transfer
    - ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ì˜ Taskì— non-triviallyí•˜ê²Œ transfer
        - íŠ¹ìˆ˜í•œ ë°ì´í„° ì…‹ìœ¼ë¡œ í›ˆë ¨í•˜ì§€ ì•Šì•„ë„ ì§€ë„í•™ìŠµê³¼ë„ ê²½ìŸë ¥ìˆëŠ” ì„±ëŠ¥

## 1. Introduction and Motivating Work

- text-to-text
    - NLP ëª¨ë¸ì€ web-scale
- Vision : crowd-labeled dataset (ImageNet ë“±)

= ì›¹ í…ìŠ¤íŠ¸ ì‚¬ì „í•™ìŠµ ë°©ë²•ì„ visionì— ì ìš©?!

- ì´ì „ ëª¨ë¸
    - (1999) ë¬¸ì„œì˜ ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ - ì´ë¯¸ì§€ pair í•™ìŠµ
    - (2007) predict words in caption associated with images
    - (2012) low-level image and text tag features
    - (2016) CNNs trained to predict words in image captions
    - (2016) YFCC100M dataset : multi-label classification task (AlexNet ê¸°ë°˜)
    - (2017) Imagenet â€¦
    - (2020)Â **VirTex, ICMLM, Con-VIRT**Â ê¸°ë°˜ ëª¨ë¸ ë“±ì¥ (contrastive objectives to learn image representations from text)
- CLIP íŠ¹ì§• ë° ë³¸ ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ
    - ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ë¡œ í›ˆë ¨ (ì›¹ ê¸°ë°˜) : 4ì–µ
    - Contrastive Language-Image Pre-training
    - 30ê°€ì§€ì˜ ë°ì´í„° ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ - í…ŒìŠ¤í¬ì— êµ¬ì• ë°›ì§€ ì•Šê³  ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
        - zero-shot ìœ¼ë¡œ ìˆ˜í–‰í•œ ë¶„ë¥˜ í…ŒìŠ¤í¬ì— ëŒ€í•´ì„œ ê¸°ì¡´ ì§€ë„í•™ìŠµ ë³´ë¸ë³´ë‹¤ ì„±ëŠ¥ ì¢‹ìŒ

## 2. Approach

### 2.1 Natural Language Supervision

- ìì—°ì–´ëŠ” í™•ì¥í•˜ê¸° ì‰½ë‹¤. ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ í˜•íƒœë¡œ ìˆ˜ë™ì ìœ¼ë¡œ ë°°ìš°ë„ë¡ í•¨.

### 2.2 Creating a Sufficiently Large Dataset

- ìì—°ì–´ ë°ì´í„°ì˜ ì£¼ ë™ê¸°ëŠ” â€œë§ì€ ì–‘ì˜ ê³µê³µ ì¸í„°ë„· ë°ì´í„°â€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ.
    - ì¸í„°ë„·ì—ì„œ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” 4ì–µìŒ ë°ì´í„°ì…‹ êµ¬ì„±.
    - (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒì„ ê²€ìƒ‰ â†’ 500,000ê°œì˜ ì¿¼ë¦¬ ì§‘í•© ì´ìš©í–ˆê³  ì¿¼ë¦¬ë‹¹ ìµœëŒ€ 20,000ê°œë¡œ í´ë˜ìŠ¤ ê· í˜•ì„ ì¡°ì • (WebImageText)

### 2.3 Selecting an Efficient Pre-training Method

- ì²« ì‹œë„ : VirTex (jointly trained an image CNN and text transformer from scratch to predict the caption of an image) â†’ ê°œì„  X
    - ResNet-50 image encoder ê¸°ì¡´ ëª¨ë¸ë³´ëŒ€ 3ë°° ëŠë¦¼
    - ë¶„ë¥˜ê¸°ëŠ” ì •í™•í•œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë ¤ê³  í•¨ â†’ ì–´ë ¤ì›€.
- ëŒ€ì¡°ì  í‘œí˜„ í•™ìŠµì„ í†µí•´ì„œ ë” ë‚˜ì€ í‘œí˜„ ê°€ëŠ¥
- ê³ í’ˆì§ˆ ì´ë¯¸ì§€ í‘œí˜„ í•™ìŠµ ê°€ëŠ¥í•˜ë‚˜ ì»´í“¨í„° ìì›ì´ ëª‡ë°° í•„ìš”í•¨.
- bag-of-word ë‹¨ìœ„
- N(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒ ë°°ì¹˜ê°€ ì£¼ì–´ì§€ë©´ CLIPì€ ë°°ì¹˜ ì „ì²´ì—ì„œ N Ã— N ê°€ëŠ¥í•œ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒ ì¤‘ ì–´ë–¤ ê²ƒì´ ì‹¤ì œë¡œ ë°œìƒí–ˆëŠ”ì§€ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨
    - CLIPëŠ” ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ê³µë™ìœ¼ë¡œ í›ˆë ¨ì‹œì¼œ Nê°œì˜ ì‹¤ì œ ìŒì˜ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”©ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì„ ê·¹ëŒ€í™” â†” N^2-N ê°œ ì˜ëª»ëœ ìŒì˜ ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì†Œí™”

![](https://blog.kakaocdn.net/dn/UOAz6/btssT5M0zQr/VQvNxVCawzYbw2fX1367H0/img.png)

Â  Â  - CLIPì€ ë°ì´í„° ì„¸íŠ¸ê°€ ì»¤ì„œ overfittingì´ ë°œìƒí•˜ì§€ ì•ŠìŒ. (ì£¼ìš” ë¬¸ì œëŠ” ì•„ë‹˜). ë¹„ì„ í˜• ë°©ì‹ë„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.  
Â  Â  - We train CLIP from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights.  
Â  Â  - psudocode

![](https://blog.kakaocdn.net/dn/k7Om2/btss3Y0k31C/7KtpuqfkVGELWCQ1TdC801/img.png)

### 2.4 Choosing and Scaling a Model

- ì´ë¯¸ì§€ ì¸ì½”ë”
    - ResNet-50
        - ResNetD ê°œì„ , antialiased rect-2 blur pooling, global average pooling layer with an attention pooling, attention pooling is implemented as a single layer of â€œtransformer-styleâ€ multi-head QKV attention
    - Vision Transformer (ViT)
        - We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.
        - transformer ì´ì „ì— ê²°í•©ëœ íŒ¨ì¹˜ ë° ìœ„ì¹˜ ì„ë² ë”©ì— ë ˆì´ì–´ ì •ê·œí™”ë¥¼ ì¶”ê°€í•˜ëŠ” ìˆ˜ì •ë§Œ í–ˆìŒ.
- í…ìŠ¤íŠ¸ ì¸ì½”ë” (ê¹Šì´ ì¡°ì •X)
    - Transformer
        - As a base size we use a 63M-parameter 12-layer 512-wide model with 8 attention heads.
        - The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size
        - [SOS] [EOS] í† í° í‘œí˜„

### 2.5 Training

- 5 ResNets and 3 Vision Transformers í›ˆë ¨
    - RN50x4, RN50x16 ë° RN50x64
    - ViT-B/32, ViT-B/16 ë° ViT-L/14
- Adam Optimizer & cosine schedule
- RN50x64ëŠ” 592ê°œì˜ V100 GPUì—ì„œ êµìœ¡í•˜ëŠ” ë° 18ì¼ ì†Œìš”
- ìµœëŒ€ ê·œëª¨ì˜ Vision TransformerëŠ” 256 V100ì—ì„œ 12ì¼ì´ ì†Œìš”

## 3. Experiments

### 3.1 Zero-Shot Transfer

1) Motivation

- task-learningì— ì´ˆì ì„ ë§ì¶¤ (evaluates performance on a task on a specific distribution)
- ì‹œê° N-ê·¸ë¨(Visual N-grams)
    - N-ê·¸ë¨ì€ ì›ë˜ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒìœ¼ë¡œ, ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ nê°œì˜ ì—°ì†ëœ ìš”ì†Œ(ë‹¨ì–´ ë˜ëŠ” ë¬¸ì)ë¡œ ì´ë£¨ì–´ì§„ ì‹œí€€ìŠ¤ë¥¼ ì˜ë¯¸.
    - ì»´í“¨í„° ë¹„ì „ì˜ ë§¥ë½ì—ì„œ ì‹œê° N-ê·¸ë¨ì€ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ì‹œê°ì  ìš”ì†Œ ë˜ëŠ” íŠ¹ì§•ë“¤ì˜ ì—°ì†ëœ ì‹œí€€ìŠ¤ë¥¼ ë§í•¨.

2) Using CLIP for zero-shot transfer

- we use the names of all the classes in the dataset as the set of poten- tial text pairings and predict the most probable (image, text) pair according to CLIP.

3) Initial comparison to Visual N-Grams

- ë™ì¼í•œ ì¡°ê±´ì—ì„œ í›ˆë ¨í•œ ê²ƒì´ ì•„ë‹ˆë¯€ë¡œ ë™ì¼í•œ ì„ ìƒì—ì„œ ë¹„êµí•˜ê¸°ì—ëŠ” ì–´ë ¤ì›€.
- In total we expand from the 3 datasets reported in Visual N- Grams to include over 30 datasets and compare to over 50 existing computer vision systems to contextualize results.

4) Prompt engineering and ensembling

#### <ë¬¸ì œ>

- A common issue is polysemy. When the name of a class is the only information provided to CLIPâ€™s text encoder it is unable to differentiate which word sense is meant due to the lack of context. In some cases multiple meanings of the same word might be included as different classes in the same dataset!
- Another issue we encountered is that itâ€™s relatively rare in our pre-training dataset for the text paired with the image to be just a single word. Usually the text is a full sentence describing the image in some way.

#### <í•´ê²°ë°©ì‹>

- To help bridge this distribution gap, we found that using the prompt template â€œA photo of a {label}.â€
    - â€œA photo of a {label}, a type of pet.â€
    - â€œa satellite photo of a {label}.â€
    - â€A photo of a big {label}â€ and â€œA photo of a small {label}â€.

5) Analysis of zero-shot CLIP performance

![](https://blog.kakaocdn.net/dn/rjxwz/btssVIRhYqn/N2FFt0oxRFhykjIqJc6Hqk/img.png)

- 27ê°œ ë°ì´í„°ì…‹ì—ì„œ 16ê°œê°€ ë” ë†’ì€ ì„±ëŠ¥
- ImageNetì˜ ëª…ì‚¬ ì¤‘ì‹¬ < ë™ì‚¬ë¥¼ í¬í•¨í•˜ëŠ” ì‹œê°ì  í‘œí˜„ ë” ë†’ì€ ì„±ëŠ¥ìœ¼ë¡œ ì¶”ì¸¡
- "ì¼ë°˜" ê°œì²´ ë¶„ë¥˜ ë°ì´í„° ì„¸íŠ¸ì—ì„œëŠ” ëª¨ë“  ê²½ìš°ì— ì œë¡œìƒ· CLIP ë” ìš°ì„¸

â†” ë°˜ëŒ€ë¡œ ë¶„ë¥˜ ì˜ ëª»í•œ ê²½ìš°ë¥¼ ë³´ë©´ êµ¬ì²´ì , ë³µì¡í•œ í…ŒìŠ¤í¬ (ìœ„ì„± ì´ë¯¸ì§€ ë¶„ë¥˜, ë¦¼í”„ì ˆ ì¢…ì–‘ íƒì§€ ë“±)

![](https://blog.kakaocdn.net/dn/bxIAYX/btssS1K4n4v/qEhN779gdgw1P7AvMXFYE0/img.png)

- ì œë¡œìƒ·ì´ ì›ìƒ·ë³´ë‹¤ ì„±ëŠ¥ì´ ë†’ìŒ
    - ì²«ì§¸, CLIPì˜ ì œë¡œìƒ· ë¶„ë¥˜ê¸°ëŠ” ì‹œê°ì  ê°œë…ì„ ì§ì ‘ ì§€ì •("ì†Œí†µ")í•  ìˆ˜ ìˆëŠ” ìì—°ì–´ë¥¼ í†µí•´ ìƒì„±ë©ë‹ˆë‹¤. ëŒ€ì¡°ì ìœ¼ë¡œ, "ì •ìƒì ì¸" ì§€ë„ í•™ìŠµì€ í›ˆë ¨ ì‚¬ë¡€ì—ì„œ ê°œë…ì„ ê°„ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ë§¥ ì—†ëŠ” ì˜ˆì œ ê¸°ë°˜ í•™ìŠµì€ íŠ¹íˆ ì›ìƒ· ì‚¬ë¡€ì—ì„œ ë§ì€ ë‹¤ë¥¸ ê°€ì„¤ì´ ë°ì´í„°ì™€ ì¼ì¹˜í•  ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
- ì œë¡œìƒ·ì´ ë‹¤ë¥¸ 16 í´ë˜ìŠ¤ë¡œ í›ˆë ¨ëœ (ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€) ëª¨ë¸ì˜ ì •í™•ë„ì™€ ìœ ì‚¬í•œ ê²°ê³¼

![](https://blog.kakaocdn.net/dn/nBRhP/btssTgVML9f/5OwRFEWFkeAyCMkkkddasK/img.png)

- ì™„ì „ ì§€ë„í•™ìŠµìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ê³¼ì˜ ë¹„êµ
    - ì œë¡œìƒ· ì„±ëŠ¥ê³¼ ì™„ì „íˆ ê°ë…ëœ ì„±ëŠ¥ ì‚¬ì´ì—ëŠ” 0.82(p-ê°’ < 10-6)ì˜ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ ìˆìœ¼ë©°, CLIPëŠ” ê¸°ë³¸ í‘œí˜„ê³¼ ì‘ì—… í•™ìŠµì„ ì œë¡œìƒ· ì „ì†¡ì— ì—°ê²°í•˜ëŠ” ë° ë¹„êµì  ì¼ê´€ì„±ì´ ìˆìŒ

![](https://blog.kakaocdn.net/dn/bO5KRn/btssSo7Cjw6/KOxXOKsZaSIWQhKOlyf241/img.png)

- í›ˆë ¨ì»´í“¨íŒ…ì˜ ì¦ê°€ëŠ” ì„±ëŠ¥ ì¦ê°€ì— ì˜í–¥?
    - The GPT family of models has so far demonstrated consis- tent improvements in zero-shot performance across a 1000x increase in training compute.
- í•´ë‹¹ ê´€ê³„ëŠ” í™•ì‹ í•  ìˆ˜ ì—†ìŒ.