---
title: 빅데이터 전처리를 위한 기술 배우기
description: 하둡, 하이브와 스파크
author: annmunju
date: 2025-06-02 15:32:00 +0900
categories: [Ingest-web-click-log, study]
tags: [hadoop, hive, spark]
pin: false
math: true
mermaid: true
comments: true
---

> 대용량 데이터를 처리하기 위한 여러 도구

이제껏 경험해본 데이터 전처리 프레임워크는 pandas 뿐이었습니다. 데이터 엔지니어가 데이터를 처리할 때는 주로 빅데이터를 분산 저장, 처리하는 도구인 hadoop, spark를 처음부터 제대로 이해하기 위해서 먼저 개념을 익히고자 합니다.

## 들어가며 : 오늘의 목표

1. 하둡이 뭔지 이해하기 -> 그리고 하이브도 같이 이해하기
2. 스파크가 뭔지 이해하기

---

## 1. Hadoop과 Hive 이해  

### 1.1. Hadoop

아파치 하둡은 대용량 데이터를 분산 저장, 처리하기 위한 오픈소스 프레임워크입니다. 단일 서버가 감당하기 어려운 데이터를 저가형 서버==노드에 나눠 저장하고 MapReduce 같은 분산 알고리즘을 통해 병렬 처리할 수 있도록 설계되었습니다.

- 구성 요소
    - **하둡 파일시스템 (HDFS)**: 블록 단위로 나눠 여러 노드에 복제하여 저장할 수 있는 분산 파일 시스템
    - **YARN (Yet Another Resource Negotiator)**: 클러스터 전체 자원(CPU, 메모리 등)을 관리하고 작업(Job)을 배정·스케줄링하는 Resource Manager 역할
    - **MapReduce 모델**: 하둡이 처음 나왔을 때 배치 처리의 표준으로 자리 잡은 분산 프로그래밍 모델. Map 단계에서 키-값으로 변환해 분산처리하고 reduce 단게에서 map 단계 결과를 모아 최종 집계, 처리함.

>  
- HDFS는 데이터를 작은 블록으로 나눠 여러 곳에 복제해서 저장 → 노드 장애 시 데이터 유실을 방지  
- 병렬 읽기/쓰기로 대용량 파일을 빠르게 처리  
- NameNode(메타데이터)와 DataNode(실제 저장)로 역할이 분리되어 있음  
{: .prompt-tip}

### 1.2. Hive

Apache Hive는 HDFS에 저장된 대용량 데이터를 SQL처럼 간단하게 조회·분석할 수 있게 해 주는 데이터웨어하우스 도구입니다. 내부적으로 HiveQL 쿼리를 MapReduce/Tez/Spark 작업으로 변환해 분산 처리합니다.

- **Hive 메타스토어와 테이블 구조:**  
    - 메타스토어는 테이블 이름·컬럼 정보·파티션 정보·파일 위치(HDFS 경로) 같은 메타데이터를 RDBMS에 저장  
    - EXTERNAL 테이블과 MANAGED 테이블 두 가지 종류가 있으며, 파티션을 활용해 데이터 디렉터리를 나누어 저장  
- **HiveQL로 데이터 조회하기:**  
    - SQL과 거의 동일한 문법인 HiveQL을 사용  
    - 단일 쿼리를 내부적으로 분산 작업(MapReduce/Tez/Spark)으로 변환하여 실행  
    - 테이블 생성, 파티션 관리, SELECT·GROUP BY·JOIN 등 기본 SQL 기능 지원  
- **Hive와 Hadoop의 연동 방식:**  
    - HiveServer2가 메타스토어에서 HDFS 저장 위치를 조회  
    - 실행 엔진(MapReduce/Tez/Spark)이 HDFS 파일을 분산 읽어 처리  
    - 결과를 임시 HDFS에 저장한 뒤 HiveServer2가 클라이언트에 반환  

>
- Hive 메타스토어는 테이블 스키마→HDFS 경로를 관리 → SQL 쿼리를 분산 작업으로 변환  
- 파티셔닝을 사용하면 특정 디렉터리만 읽어 쿼리 성능을 높일 수 있음  
- HiveQL 엔진(MapReduce vs. Tez vs. Spark)을 변경해 성능 최적화 가능 
{: .prompt-tip}

---

### 1.3. Hadoop / Hive 사용 사례와 장단점

#### 사용 사례

| 사용 사례             | 설명                                                     |
| ------------------ | ------------------------------------------------------ |
| 로그 데이터 배치 분석    | 하루치 클릭스트림·서버 로그 등 대량 파일을 MapReduce나 HiveQL로 집계          |
| 데이터웨어하우스 구축    | HDFS에 저장된 데이터를 Hive 테이블로 정의해 BI 도구에서 SQL로 대화형 조회         |
| 대규모 ETL           | 여러 소스(MySQL, CSV, 로그 등) → HDFS 적재 → MapReduce/HiveQL로 변환·적재    |
| 배치 머신러닝 전처리    | HDFS 원본 데이터를 MapReduce로 피처 생성 → Hive 테이블로 저장 → Spark ML 학습 |

#### 장단점

| 구분    | 장점                                                                    | 단점                                                                        |
| ----- | ----------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| Hadoop | - 노드를 추가하면 용량·연산 능력 확장 가능<br>- 블록 복제로 내결함성 확보<br>- 오픈소스 에코시스템 연동 용이 | - 디스크 기반 처리로 반복 연산 시 속도 느림<br>- 실시간 분석 부적합<br>- 여러 서비스 운영 부담        |
| Hive   | - SQL 친화적으로 대용량 데이터 처리<br>- 파티셔닝으로 쿼리 성능 개선<br>- HiveQL→MapReduce/Tez/Spark 자동 변환 | - 기본 MapReduce 엔진 사용 시 실행 지연<br>- 복잡한 UDF 개발 필요 시 진입 장벽 상승<br>- 실시간 처리 불가 |

이처럼 Hadoop은 대용량 데이터를 안정적으로 분산 저장·처리하는 기반을 제공하고 Hive는 그 위에서 SQL처럼 손쉽게 데이터를 조회·분석할 수 있게 해주는 도구입니다.  

---

## 2. Spark

필요한 데이터를 메모리에 올려두고 계산하여 읽고 쓰기를 빠르게 처리할 수 있는 엔진입니다. 

### 2.1. Spark 개요  

스파크는 다음과 같은 특징이 있습니다.
- 필요한 데이터를 메모리(컴퓨터 RAM)에 올려두고 계산하니까 하둡처럼 디스크에서 계속 읽고 쓰는 것보다 **훨씬 빨라요**.
- 배치 작업도 물론 할 수 있지만, **실시간에 새로운 데이터가 들어오면 바로바로 처리해서 결과를 보여줄 수 있는 기능**(Structured Streaming)이 있어요.
    - 클릭 이벤트가 Kafka 같은 메시지 큐에 들어오자마자 현재 시점까지의 집계값을 끊임없이 갱신해 대시보드에 바로 띄울 수 있어요.
- 이외에도 다양한 분석/응용 기능이 있어요.
    - SQL처럼 테이블 형태로 데이터를 다룰 수 있는 **Spark SQL**
    - 머신러닝 알고리즘을 쉽게 사용하게 해주는 **MLlib**
    - 그래프 알고리즘을 지원하는 **GraphX** 등
    - 이 모든 기능을 하나의 통합된 환경에서 거의 동일한 코드 스타일(PySpark, Scala, R 등)로 쓸 수 있어요.

---

### 2.2. Spark 아키텍처

1. **Driver와 Executor 구조**  
   - **Driver**는 Spark 애플리케이션의 중심 역할을 해요.  
     - 프로그램의 메인 메서드를 실행하면서, 작업을 여러 개의 태스크(Task)로 나누고 전체 실행 계획(DAG)을 생성합니다.  
     - 실행 계획을 클러스터 매니저에게 전달해 자원을 요청하고, 할당된 Executor에게 작업을 분배합니다.  
     - 작업 진행 상황을 모니터링하고, 최종 결과를 모아 사용자가 원하는 형태로 돌려줍니다.  
   - **Executor**는 실제 데이터를 계산하는 워커 입니다.  
     - 클러스터 매니저가 할당한 노드(NodeManager)에 JVM 프로세스로 올라가며, Driver가 준 Task를 병렬로 실행해요.  
     - 입력 데이터를 읽어서 각 Task별로 연산(맵, 필터, 집계 등)을 수행하고, 결과를 메모리(혹은 디스크)에 저장합니다.  
     - 작업이 끝나면 Driver에게 결과나 중간 상태를 보고하고, 필요 시 셔플(shuffle) 과정을 통해 다른 Executor와 데이터를 주고받습니다.

2. **클러스터 매니저(YARN 모드) 연동**  
   - Spark는 여러 종류의 클러스터 매니저를 지원하지만, Hadoop 환경에서는 **YARN**을 많이 사용합니다.  
   - **Spark on YARN** 동작 흐름:  
     1. 사용자가 `spark-submit --master yarn` 명령으로 애플리케이션을 제출  
     2. YARN ResourceManager(RM)는 새로운 **ApplicationMaster(AM)** 컨테이너를 실행하여 Driver 프로세스를 시작  
     3. AM(Driver)은 RM에게 Executor 몇 개, CPU 몇 개, 메모리 몇 GB를 할당해 달라는 요청을 보냄  
     4. RM이 각 NodeManager(NM)에게 자원 할당을 지시 → Executor 컨테이너가 노드별로 실행  
     5. Driver는 할당된 Executor에 Task를 분배하여 병렬 처리 수행  
     6. 작업 완료 후 AM이 RM에 작업 종료를 알리고, RM이 할당된 자원을 회수  

>
- **Driver**는 애플리케이션의 실행 계획(DAG) 작성과 Task 분배를 담당하는 뇌 역할  
- **Executor**는 Driver가 분배한 Task를 실제로 메모리 기반으로 실행하는 일꾼 역할  
- **Spark on YARN** 모드에서는 ResourceManager가 ApplicationMaster(Driver)를 띄우고, NodeManager가 Executor 컨테이너를 실행  
- YARN이 전체 클러스터 자원을 관리해주므로, Spark 작업과 다른 Hadoop 작업이 충돌 없이 함께 실행될 수 있음  
- 노드를 추가하면 더 많은 Executor를 할당받아 병렬 처리 성능을 쉽게 확장할 수 있음  
{: .prompt-tip}

---

### 2.3. Spark vs. Hadoop 비교

| 비교 항목                  | Hadoop (MapReduce)                                                                 | Spark                                                                                              |
| ----------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| **처리 속도 및 메모리 활용**   | - 디스크 기반 처리 → 각 단계마다 디스크 읽기/쓰기 발생<br>- 메모리 캐싱 불가 → 반복 연산 시 속도 느림      | - 메모리 기반 처리 → 데이터를 메모리에 올려두고 연산<br>- 반복 연산 시 추가 디스크 I/O 없이 빠른 처리 가능 |
| **배치 방식 vs. 스트리밍 방식** | - 순수 배치 전용 모델<br>- 하루치 데이터를 몰아서 처리에만 적합<br>- 실시간 스트리밍 처리 불가             | - 배치와 스트리밍(Structured Streaming) 모두 지원<br>- 실시간으로 들어오는 데이터 거의 실시간 처리 가능      |
| **데이터웨어하우스 연동 관점**   | - Hive, HBase 등 Hadoop 에코시스템과 연동<br>- HiveQL→MapReduce로 대용량 배치 쿼리 가능                   | - Spark SQL을 통해 Hive 메타스토어 테이블 바로 조회 가능<br>- Parquet/ORC 등 컬럼 형식 파일을 직접 읽고 쓰기 용이 |

---

## 정리하기

**Hadoop**과 **Hive**를 통해 대용량 데이터를 분산 저장·관리하고 SQL처럼 조회하는 도구를 설명했고 **Spark**를 이용해 메모리 기반으로 빠르게 배치·실시간 처리를 수행하는 내용까지 살펴보았습니다.

Hadoop은 HDFS와 YARN으로 데이터 안전성·확장성을 보장하고, Hive는 메타스토어를 통해 테이블·파티션 정보를 관리하며 복잡한 MapReduce 작업을 추상화합니다. 반면 Spark는 Driver와 Executor 구조로 메모리에 데이터를 올려 반복 연산을 최적화하고, Structured Streaming으로 실시간 데이터 파이프라인을 구성할 수 있습니다. 

다음에는 EC2 위에 Airflow, Spark Cluster, Hive/Hadoop, HDFS DataNode를 실제로 올려 하루마다 S3 로그를 Spark로 처리하고 Hive에 적재하는 구체적인 워크플로우 계획을 세워보겠습니다.