---
title: 웹 클릭스트림 수집하기 4) EC2 카프카 & 컨테이너 Lambda 배포
description: 카프카를 프라이빗으로 전환하고 Lambda로 프로듀서 붙이기
author: annmunju
date: 2025-05-29 11:54:00 +0900
categories: [Ingest-web-click-log, poc]
tags: [kafka, lambda, s3]
pin: false
math: true
mermaid: true
comments: true
---

> 이제 클라우드 환경에서 람다로 데이터 수집 본격화

이번 주에는 EC2에 띄운 Kafka 클러스터를 기반으로 GitHub Pages에 호스팅된 프론트엔드에서 발생하는 클릭스트림 로그를 AWS Lambda로 수집하는 프로세스를 완성할 계획입니다.  
Lambda 프로듀서 함수가 Kafka 토픽에 메시지를 전송하면, 동일한 EC2에서 실행 중인 Kafka Connect S3 Sink 커넥터가 해당 토픽을 소비해 S3에 배치 적재하는 end-to-end 파이프라인을 구축해봅니다.  

---

## 아키텍처 그리기

![단일 클러스터 아키텍처](sources/project1_Ingest-web-click-log/2025-05-28-람다-S3-저장/01.png)

1. **클라이언트 → API Gateway**
    - GitHub Pages(Jekyll)로 호스팅된 프론트엔드에서 JS 클릭스트림 이벤트가 발생하면 퍼블릭 API Gateway 엔드포인트로 HTTP 호출이 들어옵니다.
    - API Gateway는 Lambda Kafka Producer 함수를 트리거합니다.
2. **Lambda Producer**
    - 함수 코드가 Kafka 클러스터의 토픽(clickstream)에 메시지를 프로듀싱합니다.
3. **EC2 Kafka 클러스터**
    - EC2 인스턴스 위에서 Zookeeper + Kafka Broker가 함께 동작합니다.
    - 이 클러스터가 프로듀서가 보낸 메시지를 저장·관리합니다.
4. **Kafka Connect S3 Sink** (동일 EC2)
    - EC2에서 Kafka Connect를 실행하고 S3 Sink 커넥터를 설정합니다.
    - 이 커넥터가 clickstream 토픽을 소비해 지정된 주기(Flush size)마다 S3 버킷에 JSON/Parquet 파일로 배치 적재합니다.
5. **Amazon S3**
    - 최종적으로 적재된 로그 데이터는 S3 버킷에 저장되어, 후속 분석 및 시각화에 활용할 예정입니다.

---

## 🎯 이번 글 목표

다음과 같은 아키텍처를 기반으로, 오늘은 ec2에 띄운 카프카의 설정을 마치고 lambda를 배포해야합니다.
1. Lambda Producer 배포하기
    - 컨테이너 형태로 배포하기 (with ECR)
2. EC2 카프카 배포하기
    - 퍼블릭 서브넷 EC2에 카프카 설정을 끝내고
    - S3 Sink 커넥터를 설정하기
3. S3 버킷 생성하기
    - 버킷 생성을 위한 데이터 저장 방식 결정
    - 버킷 보호를 위한 규칙 설정

---

### 1. Lambda Producer 생성하기

람다에 kafka 프로듀서 코드를 도커 컨테이너 이미지로 작성하고 ECR로 올려보겠습니다.

#### a. ecr과 lambda 만들 terraform 파일작성

a. ECR 리포지토리 및 Lambda 생성용 Terraform 파일 작성  
b. Terraform으로 ECR & Lambda 인프라 프로비저닝  
c. Dockerfile 작성 및 컨테이너 이미지 빌드  
   1. 베이스 이미지 선택 (slim vs Lambda RIC)  
   2. librdkafka 설치  
   3. Python 패키지 & Mangum 설치  
   4. ENTRYPOINT/CMD 설정  
d. 이미지 태그 & ECR 푸시  
   1. `aws ecr get-login-password | docker login`  
   2. `docker build -t kafka-producer …`  
   3. `docker tag …` / `docker push …`  
e. Lambda 함수 업데이트 & 동작 검증  
   1. Terraform 재실행 (`terraform apply`)  
   2. 테스트 이벤트로 호출하기 (AWS CLI / 함수를 직접 호출)  

---

### 2. EC2 카프카 배포하기  

a. EC2에서 Kafka 설정 마무리  
   - 서비스 재시작 & 토픽 생성 확인  
b. 보안그룹·네트워크 검증  
   1. SG 인그레스/아웃그레스  
   2. 내부에서 토픽 조회/produce·consume 테스트  

---

### 3. S3 버킷 생성하기  
a. 저장 방식 & 네이밍 컨벤션 결정  
   1. 폴더 구조 (year=/month=/day= 등)  
   2. 오브젝트 키 패턴  
b. Terraform으로 S3 버킷 정의  
   1. aws_s3_bucket 리소스  
   2. 라이프사이클 / 버전 관리 설정  
c. 버킷 정책 및 IAM 권한 설정  
   1. PutObject, GetObject 권한  
   2. S3 Block Public Access  
e. 최종 검증  
   1. 버킷에 파일 업로드 테스트  
   2. 권한/정책 적용 확인